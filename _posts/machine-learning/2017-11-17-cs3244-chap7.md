---
layout: post-page
title: Bias and Variance
description: Includes toimages on bias, variance, noise and overfitting
imgsrc: assets/images/cs3244/2.png
banner: assets/images/cs3244/ml_banner.jpg
date: 2017-11-17
category: machine-learning
tags: [Machine Learning]
author: Song Jiaming
---

This article is a summary note for CS3244 week 7 content. The lecture notes of this course used Caltech's notes as reference.

## Approximation-Generalization Tradeoff
Suppose we have a set of hypothesis functions __H__ to approximate __f__. The ideal case is when __H={ f }__. However, that's unlikely to happen.
<br>
<br>
To be more realistic, we hope our __H__ have a small $E_{out}$, which means __H__ is a good approximation of __f__ for unseen sample (out-of-sample). However, there is a tradeoff:
- Complex __H__ (more hypothesis) → better chance of approximating __f__
- Simple __H__ (less hypothesis)→ better chance of generalizing unseen sample

<br>
<br>

---


## Bias-Variance Analysis
We need a way to __quantify the above tradeoff__.  The bias-variance analysis is one way,([VC Analysis]({{site.baseurl}}/posts/cs3244-chap6.html) is another way) it decomposes $E_{out}$ into 2 components:
1. How well __H__ can approximate __f__ overall
2. How well we can zoom in on a good __h ∈ H__

<br>
__Approach__: we apply our __H__ to data and obtain a result, compare this result with the real-valued targets (the true result of data), then use squared error to quantify the difference.
- Start with $E_{out}$:
    - Let __g ∈ H__ be a hypothesis, __D__ be a dataset which we trained our __g__ on, __x__ be another dataset which we use __g__ to predict its results.<br>
    - Then $E_{out}(g^{(D)}) = E_{x}[ (g^{(D)}(x) - f(x))^{2}]$

<br>
- We want to generalizing over all N sets of D, (expectation of D)
    - $E_{D}[E_{out}(g^{(D)})] = E_{D}[E_{x}[ (g^{(D)}(x) - f(x))^{2}]]= E_{x}[\bbox[pink]{(E_{D}[(g^{(D)}(x) - f(x))^{2}]} ]$

<br>
- To evaluate  $E_{D}[(g^{(D)}(x) - f(x))^{2}]$, define __average hypothesis__ $\bar{g}(x) = E_{D}[g^{(D)}(x)]$.
    + Imagine we have $D_{1},D_{2},...,D_{k}$, then $\bar{g}(x) \approx \frac{1}{K}\sum\limits_{k=1}^{K} g^{(D)}(x)$

<br>
- Then,
\begin{align} E_{D}[(g^{(D)}(x) - f(x))^{2}] &= E_{D}[(g^{(D)}(x)- \bar{g}(x) + \bar{g}(x) - f(x))^{2}] \\\\ &= E_{D}[(\bbox[lightblue]{g^{(D)}(x)- \bar{g}(x)})^{2} + (\bbox[lightGreen]{\bar{g}(x) - f(x)})^{2} + 2(\bbox[lightblue]{g^{(D)}(x)- \bar{g}(x)}\,)(\,\bbox[lightGreen]{\bar{g}(x) - f(x)})] \end{align}

<br>
- Since $E_{D}(2(\bbox[lightblue]{g^{(D)}(x)- \bar{g}(x)}\,)(\,\bbox[lightgreen]{\bar{g}(x) - f(x)})) = (E_{D}[g^{(D)}(x)]- \bar{g}(x))(E_{D}[\bar{g}(x) - f(x)]) = (\bar{g}(x)-\bar{g}(x))\cdot C$
    - C is a constant

<br>
* $\bbox[pink]{E_{D}[(g^{(D)}(x) - f(x))^{2}]} = \bbox[lightblue]{E_{D}[(\bar{g}(x) - f(x))]^{2}} + \bbox[lightgreen]{(\bar{g}(x) - f(x))^{2}}$
    - The 1st part highlighted in pink: how far away __your hypthesis__ (on the specific D dataset) is from the true __f__
    - The 2nd part highlighted in blue: how far away is __your hypothesis__ (on the specific D dataset) from the __best possible hypothesis__ you can have (on all Ds)
        + This is the __var(x)__
        + Because this measures how far each hypothesis on each D is different from the best hypothesis 
    - The 3rd part highlighted in green: how far away your __best possible hypothesis__ is from the true __f__
        - This is also the __bias(x)__
        + Because that's how the best hypothesis set is biased away from __f__
        
<br>
+ Finally,  $E_{D}[E_{out}(g^{(D)})] = E_{x}[bias(x)+var(x)]$ = __bias + var__

### Tradeoff
We want to argue that when one of bias or var goes up, the other one goes down.
<br>

- Bias = $E_{x}[(\bar{g}(x) - f(x))^{2}]$
- Variance = $E_{x}[E_{D}[(\bar{g}(x) - f(x))]^{2}]$

<br>
As seen in the figure below:
- When only 1 __H__ exists, the distance is far from f. When there are more __H__, on average, they are nearer to f than just 1 single H.
- However, for more __H__, $\bar{g}$ can be at anywhere, where for 1 __H__, it is the only $\bar{g}$

__Thus, As H increases, bias decreases, but variance increases.__
![Bias-Variance]({{site.baseurl}}/assets/images/cs3244/chap7/bias_var.png)

<br>
### Example: Sine targets
![sine]({{site.baseurl}}/assets/images/cs3244/chap7/sine.png)
Though $H_{0}$ has high $E_{out}$, it has lower variance, so always match 'model complexity' to the data resources, not to the target complexity.

<br>
<br>

---

## Learning Curve
Given a dataset D of size N, we have:
- Expected out-of-sample error: $E_{D}[E_{out}(g^{(D)})]$
- Expected in-sample error: $E_{D}[E_{in}(g^{(D)})]$

<br>
Learning curve is to plot the relationship of how these errors vary with N.
- For simple model, the expected error is higher, but discrepancy between 2 errors is lower

![learning curve]({{site.baseurl}}/assets/images/cs3244/chap7/lc.png)

<br>
### Linear Regression Case
- Noisy target y = $(w^{\ast})^{T}x$ + noise
- Dataset D = ${(x_{1},y_{1}),...(x_{N},y_{N})}$
- Linear regression solution: $w = (X^{T}X)^{-1}X^{T}y$
- In-sample error vector = __Xw - y__
- 'Out-of-sample':
    + Generate another target y' = $(w^{\ast})^{T}x$ + noise'
    + Error vector = __Xw - y'__
    
![learning curve]({{site.baseurl}}/assets/images/cs3244/chap7/lr.png)

<br>
<br>

---

## Overfitting

### Illustration
Suppose we have 5 data points,
- A simple target function does not fit them perfectly, there must be noise
- A 4th-order polynomial function fit them perfectly, $E_{in}=0$ but $E_{out}$ is huge ⇒ overfitting
- Reason: if 3rd order polynomial is used, $E_{in}$ won't be 0, but $E_{out}$ will be reduced. So overfitting happens at 4th order, not 3rd order.

__Overfitting__: fitting the data more than is warranted. (When $E_{in}$ goes down, $E_{out}$ goes up.) It is fitting the noise, it assume noise as a pattern in your sample, which is of course not the case in out-of-sample
<br>

### Case Study
- We first create dataset by using the 2 polynomials.
- Then, choose another 2 functions，2nd order and 10th order to fit the datasets in the left figure.
![Overfitting]({{site.baseurl}}/assets/images/cs3244/chap7/overfit.png)
<br>

__Irony of the 2 learners: O (overfitting) and R (restricted)__<br>
We only tell the 2 learners that the target function f is a 10th-order polynomial, and you have to find it by the 15 points given
- O: why not just choose 10th order, $H_{10}$
- R: Only 15 points? I will just choose $H_{2}$ and pray for luck
- So when is $H_{2}$ better, when is $H_{10}$ better?
- Even when there's no noise, irony(overfitting) still occurs
![Overfitting2]({{site.baseurl}}/assets/images/cs3244/chap7/overfit2.png)

<br>

### The role of Noise
__A detailed experiement__<br>
Impact of noise lever and target complexity: $y = f(x) + \epsilon(x) = \bbox[pink]{\sum\limits_{q=0}^{Q_{f}} \alpha_{q} x^{q} }+ \epsilon(x)$
- $\epsilon(x) = \sigma^{2}$, the noise
- $Q_{f}$: Order of the polynomial f, implies the target complexity
- The pink part is the polynomial equation up to degree $Q_{f}$.
    - and it's normalised. (From one degree to another, they are orthogonormal to each other.)
- Dataset size: N 

<br>
__Measure Overfitting:__
- still using 2nd-order $g_{2}\in H_{0}$ and 10th order $g_{10}\in H_{10}$
- Overfit measure: $E_{out}(g_{10})- E_{out}(g_{2})$ =
    + Positive: More complex one has worse out-of-sample error ⇒ overfitting
    + Negative: More complex one did better ⇒ not overfitting
    + 0: they are the same

<br>
Running 10 millions iterations for the whole process, including:
- Generate a target
- Generate a dataset
- fit both function
- look at the overfit measure

Then we obtain:
![Overfitting3]({{site.baseurl}}/assets/images/cs3244/chap7/overfit3.png)


<br>

|  Compare   |  Stochastic Noise   | Deterministic Noise |
| :----------: | :-------:  | :--------:    |
| Description   |     Fluctuations/measurement errors we cannot capture       |   The part of f we lack of capacity to capture         |
|    formula         | $y_{n} = f(x)$ + stochastic | $y_{n} = h^{\ast}(x)$+ deterministic |
| Change H   | Remain the same for different hypothesis set   | Depends on your hypothesis set |
| Re-fit the same x   |  Changes  | Stay the same |

<br>
### Noise and Bias-Variance
Recall that  $E_{D}[E_{out}(g^{(D)})]$ = __bias + var__<br>
A noisy target (instead of just f) is $y = f(x) + \epsilon(x)$, where $E[\epsilon(x)] = 0$ <br>
__A noise term__:
\begin{align} E_{D,\epsilon}[ (g^{(D)}(x)-y)^{2} ] &= E_{D,\epsilon}[ (g^{(D)}(x)-f(x) - \epsilon(x))^{2} ] \\\\ &= E_{D,\epsilon}[ (g^{(D)}(x)- \bar{g}(x) + \bar{g}(x) -f(x) - \epsilon(x))^{2} ] \\\\&= E_{D,\epsilon}[ \bbox[lightblue]{(g^{(D)}(x)- \bar{g}(x))^{2}} + \bbox[lightgreen]{(\bar{g}(x) -f(x))^{2}} + \bbox[pink]{(\epsilon(x))^{2}} + \text{cross terms} ]\end{align}
- $\bbox[lightblue]{blue}$ part is variance
- $\bbox[lightgreen]{green}$ part is bias, the deterministic noise
- $\bbox[pink]{pink}$ part is $\sigma^{2}$, the stochastic noise.

<br>
__Two Cures for Overfitting__<br>
1. Regularization: restrain the model
2. Validation: reality check by peeking at the bottom line



<br>

----

#### References
1. [Caltech Lecture 8, Bias-Variance Tradeoff](https://youtu.be/zrEyxfl2-a8)
2. [Caltech Lecture 11, Overfitting](https://youtu.be/EQWr3GGCdzw)